"""
ë¹„ë™ê¸° ì›¹ í¬ë¡¤ëŸ¬
- aiohttp ê¸°ë°˜ ë¹„ë™ê¸° HTTP ìš”ì²­
- BeautifulSoup HTML íŒŒì‹±
"""
import asyncio
import logging
from typing import List, Dict, Any, Optional
from pathlib import Path

import aiohttp
from bs4 import BeautifulSoup
from aiohttp import ClientTimeout, ClientError

# ===============================
# ì„¤ì •
# ===============================
BASE_DIR: Path = Path(__file__).resolve().parent

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.FileHandler(BASE_DIR / "crawler.log", encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ê¸°ë³¸ ì„¤ì •
DEFAULT_TIMEOUT: int = 10
DEFAULT_MAX_TOPICS: int = 5
BOARD_URL: str = "http://localhost:9090/board/list"


# ===============================
# í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤
# ===============================
class AsyncBoardCrawler:
    """ë¹„ë™ê¸° ê²Œì‹œíŒ í¬ë¡¤ëŸ¬"""

    def __init__(self, base_url: str = BOARD_URL, timeout: int = DEFAULT_TIMEOUT):
        """
        Args:
            base_url: í¬ë¡¤ë§í•  ê²Œì‹œíŒ URL
            timeout: HTTP ìš”ì²­ íƒ€ì„ì•„ì›ƒ (ì´ˆ)
        """
        self.base_url = base_url
        self.timeout = ClientTimeout(total=timeout)
        self.headers: Dict[str, str] = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        }

    async def fetch_page(self, session: aiohttp.ClientSession) -> Optional[str]:
        """
        ë¹„ë™ê¸°ë¡œ í˜ì´ì§€ HTML ê°€ì ¸ì˜¤ê¸°

        Args:
            session: aiohttp ì„¸ì…˜

        Returns:
            HTML í…ìŠ¤íŠ¸ (ì‹¤íŒ¨ ì‹œ None)
        """
        try:
            async with session.get(
                    self.base_url,
                    headers=self.headers,
                    timeout=self.timeout
            ) as response:
                response.raise_for_status()
                html: str = await response.text()
                logger.info(f"âœ… í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸° ì„±ê³µ: {self.base_url}")
                return html

        except asyncio.TimeoutError:
            logger.error(f"â° íƒ€ì„ì•„ì›ƒ: {self.base_url}")
        except ClientError as e:
            logger.error(f"âŒ HTTP ì˜¤ë¥˜: {e}")
        except Exception as e:
            logger.error(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}", exc_info=True)

        return None

    def parse_topics(self, html: str, max_topics: int = DEFAULT_MAX_TOPICS) -> List[Dict[str, str]]:
        """
        HTMLì—ì„œ í•«í† í”½ íŒŒì‹±

        Args:
            html: HTML í…ìŠ¤íŠ¸
            max_topics: ìµœëŒ€ ì¶”ì¶œ ê°œìˆ˜

        Returns:
            [{"title": "...", "content": "..."}] í˜•ì‹ì˜ ë¦¬ìŠ¤íŠ¸
        """
        topics: List[Dict[str, str]] = []

        try:
            soup = BeautifulSoup(html, 'html.parser')
            items = soup.select('table tr')[1:max_topics + 1]

            for item in items:
                tag = item.select_one('.title-link')
                if tag:
                    title: str = tag.text.strip()
                    content: str = f"ì˜¤ëŠ˜ì˜ ê²Œì‹œíŒ í•«ì´ìŠˆ, {title} ì†Œì‹ì…ë‹ˆë‹¤."

                    topics.append({
                        "title": title,
                        "content": content
                    })

            logger.info(f"âœ… íŒŒì‹± ì™„ë£Œ: {len(topics)}ê°œ ì£¼ì œ ì¶”ì¶œ")

        except Exception as e:
            logger.error(f"âŒ íŒŒì‹± ì˜¤ë¥˜: {e}", exc_info=True)

        return topics

    async def get_hot_topics(self, max_topics: int = DEFAULT_MAX_TOPICS) -> List[Dict[str, str]]:
        """
        ë¹„ë™ê¸°ë¡œ í•«í† í”½ ê°€ì ¸ì˜¤ê¸° (ë©”ì¸ ì¸í„°í˜ì´ìŠ¤)

        Args:
            max_topics: ìµœëŒ€ ì¶”ì¶œ ê°œìˆ˜

        Returns:
            í† í”½ ë¦¬ìŠ¤íŠ¸
        """
        async with aiohttp.ClientSession() as session:
            html = await self.fetch_page(session)

            if html is None:
                logger.warning("âš ï¸ ì„œë²„ ì—°ê²° ì‹¤íŒ¨ - ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜")
                return []

            return self.parse_topics(html, max_topics)


# ===============================
# í•˜ìœ„ í˜¸í™˜ í•¨ìˆ˜
# ===============================
async def get_hot_topics() -> List[Dict[str, str]]:
    """
    ë¹„ë™ê¸° í•«í† í”½ ê°€ì ¸ì˜¤ê¸° (í•˜ìœ„ í˜¸í™˜ìš©)

    Returns:
        í† í”½ ë¦¬ìŠ¤íŠ¸
    """
    crawler = AsyncBoardCrawler()
    return await crawler.get_hot_topics()


# ===============================
# CLI ì‹¤í–‰
# ===============================
async def main() -> None:
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logger.info("ğŸš€ í¬ë¡¤ëŸ¬ ì‹œì‘")

    topics = await get_hot_topics()

    if topics:
        print(f"\nâœ… ì´ {len(topics)}ê°œ í† í”½ ìˆ˜ì§‘ ì™„ë£Œ:\n")
        for idx, topic in enumerate(topics, 1):
            print(f"{idx}. {topic['title']}")
    else:
        print("âŒ ìˆ˜ì§‘ ì‹¤íŒ¨ (ì„œë²„ê°€ ì¼œì ¸ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”)")


if __name__ == "__main__":
    asyncio.run(main())
